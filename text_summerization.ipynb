{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_summerization.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVjq_ZWVP6HE",
        "outputId": "71dc8e1a-c348-420e-b2fe-733c5e26301a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "!pip install PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyDrive in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (3.13)\n",
            "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (1.7.12)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (4.1.3)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (3.0.1)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.12.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.17.4)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.17.2)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.0.3)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.2.8)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->PyDrive) (47.3.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->PyDrive) (4.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44aqx4sySbTI"
      },
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOyorhqfSky8"
      },
      "source": [
        "downloaded = drive.CreateFile({'id':\"1OxS2AuzYKl4k_x92MPioe5ctKl9uRsw0\"})   # replace the id with id of file you want to access\n",
        "downloaded.GetContentFile('review_summaries.xlsx')        # replace the file name with your file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnuQJB9z7ezD"
      },
      "source": [
        "downloaded = drive.CreateFile({'id':\"1MsdibmBi2wufR_5Ia_4VlpUkYZ8Vvz_y\"})   # replace the id with id of file you want to access\n",
        "downloaded.GetContentFile('Reviews.csv')        # replace the file name with your file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blRRemK7HIlL"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import re\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lXJPEajktn0"
      },
      "source": [
        "#df = pd.read_excel('review_summaries.xlsx')\n",
        "#df = df.reset_index(drop=True)\n",
        "#df = df.drop(df.columns[[0]], axis = 1)\n",
        "rev = pd.read_csv('Reviews.csv',nrows=55000)\n",
        "df = pd.DataFrame({\"Reviews\":rev['Text'], \"Summaries\":rev['Summary']})\n",
        "#df = df.append(temp)\n",
        "df = df.reset_index(drop = True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Fd5hMGHlSIm",
        "outputId": "e6c66fb4-c999-4d4f-a2ba-1204a25de00f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Reviews</th>\n",
              "      <th>Summaries</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I have bought several of the Vitality canned d...</td>\n",
              "      <td>Good Quality Dog Food</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
              "      <td>Not as Advertised</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>This is a confection that has been around a fe...</td>\n",
              "      <td>\"Delight\" says it all</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>If you are looking for the secret ingredient i...</td>\n",
              "      <td>Cough Medicine</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Great taffy at a great price.  There was a wid...</td>\n",
              "      <td>Great taffy</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             Reviews              Summaries\n",
              "0  I have bought several of the Vitality canned d...  Good Quality Dog Food\n",
              "1  Product arrived labeled as Jumbo Salted Peanut...      Not as Advertised\n",
              "2  This is a confection that has been around a fe...  \"Delight\" says it all\n",
              "3  If you are looking for the secret ingredient i...         Cough Medicine\n",
              "4  Great taffy at a great price.  There was a wid...            Great taffy"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NugTPrQJsufm",
        "outputId": "c8ad6779-ad68-493e-af26-3fea5a31955b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(55000, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzWVdMZu9LG6"
      },
      "source": [
        "document = df['Reviews']\n",
        "summary = df['Summaries']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGyJDqEH9xNo",
        "outputId": "039c244a-f6ad-4a24-cb0d-baa0a0cc3e1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "document[567], summary[56]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(\"After reading some of the reviews, I got nervous and opened a bag from my recent order expecting the worst! No worries here. All bags are in great shape and expiration dates aren't until June. Chips, at least from the first bag, taste like they are supposed to and all is good in the world! By the way, fifteen bags for under thirty dollars is a ton less expensive than the going price around here at the local grocery store so, yay team!\",\n",
              " 'Awesome Deal!')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ycFfQtnIcYC"
      },
      "source": [
        "Prepocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9oNcn6pIjv7",
        "outputId": "9142b67c-0342-4923-c5e0-a71759eaa118",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "# for decoder sequence\n",
        "summary = summary.apply(lambda x: '<go> ' + str(x) + ' <stop>')\n",
        "summary.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    <go> Good Quality Dog Food <stop>\n",
              "1        <go> Not as Advertised <stop>\n",
              "2    <go> \"Delight\" says it all <stop>\n",
              "3           <go> Cough Medicine <stop>\n",
              "4              <go> Great taffy <stop>\n",
              "Name: Summaries, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyC0s4cEKHqK"
      },
      "source": [
        "Tokenizing the texts into integer tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d35amb14KIaw"
      },
      "source": [
        "# since < and > from default tokens cannot be removed\n",
        "filters = '!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n'\n",
        "oov_token = '<unk>'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPnox4hpKPKU"
      },
      "source": [
        "document_tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=oov_token)\n",
        "summary_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=filters, oov_token=oov_token)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKu-BCjkKXuU"
      },
      "source": [
        "document_tokenizer.fit_on_texts(document)\n",
        "summary_tokenizer.fit_on_texts(summary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ze1jsYcXKdL_"
      },
      "source": [
        "inputs = document_tokenizer.texts_to_sequences(document)\n",
        "targets = summary_tokenizer.texts_to_sequences(summary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JviCggD1Lf4R",
        "outputId": "3cac03a1-ad25-4778-e46d-987a40a96686",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "summary_tokenizer.texts_to_sequences([\"This is a test\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[16, 23, 7, 1181]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EN9w_uyMLo0_",
        "outputId": "4e82db72-8413-49ca-9c66-6b291b79f0d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "summary_tokenizer.sequences_to_texts([[17, 23, 7, 1358]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tea is a cheesy']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEOHEkc-LrnA",
        "outputId": "3e6e82df-d0ef-4e20-cb17-bb1fac325932",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "encoder_vocab_size = len(document_tokenizer.word_index) + 1\n",
        "decoder_vocab_size = len(summary_tokenizer.word_index) + 1\n",
        "\n",
        "# vocab_size\n",
        "encoder_vocab_size, decoder_vocab_size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(45297, 11774)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1onLt1e_ME-h"
      },
      "source": [
        "Obtaining insights on lengths for defining maxlen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXWHYnUtMFqH"
      },
      "source": [
        "document_lengths = pd.Series([len(x) for x in document])\n",
        "summary_lengths = pd.Series([len(x) for x in summary])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkvnpuPuMJ1G",
        "outputId": "3b1abf04-0c7a-4ca3-92c3-85674e95ec9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "document_lengths.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    55000.000000\n",
              "mean       431.993509\n",
              "std        427.380371\n",
              "min         33.000000\n",
              "25%        181.000000\n",
              "50%        303.000000\n",
              "75%        524.000000\n",
              "max      10327.000000\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVPOeB3vMMxq",
        "outputId": "09e22526-fe7e-4c7f-dbe9-b6a83ae7d03e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "summary_lengths.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    55000.000000\n",
              "mean        35.364000\n",
              "std         13.939116\n",
              "min         13.000000\n",
              "25%         25.000000\n",
              "50%         32.000000\n",
              "75%         42.000000\n",
              "max        140.000000\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2gEIE9SMWfl"
      },
      "source": [
        "# maxlen\n",
        "# taking values > and round figured to 75th percentile\n",
        "# at the same time not leaving high variance\n",
        "encoder_maxlen = 600\n",
        "decoder_maxlen = 100\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4H8JfqPiavxX"
      },
      "source": [
        "Padding/Truncating sequences for identical sequence lengths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-c3Oj2xVaxvT"
      },
      "source": [
        "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen=encoder_maxlen, padding='post', truncating='post')\n",
        "targets = tf.keras.preprocessing.sequence.pad_sequences(targets, maxlen=decoder_maxlen, padding='post', truncating='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lN6b30xSbDWr"
      },
      "source": [
        "Creating dataset pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYrq7v87a5-Z"
      },
      "source": [
        "inputs = tf.cast(inputs, dtype=tf.int32)\n",
        "targets = tf.cast(targets, dtype=tf.int32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIBwyv0aa9OR"
      },
      "source": [
        "BUFFER_SIZE = 20000\n",
        "BATCH_SIZE = 64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9RLRL_ibOJ_"
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((inputs, targets)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIL1Mr1Ubcrc"
      },
      "source": [
        "Positional Encoding for adding notion of position among words as unlike RNN this is non-directional"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXuBA7lXgI9Q"
      },
      "source": [
        "def get_angles(position, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "    return position * angle_rates"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNgEECWngMhO"
      },
      "source": [
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(\n",
        "        np.arange(position)[:, np.newaxis],\n",
        "        np.arange(d_model)[np.newaxis, :],\n",
        "        d_model\n",
        "    )\n",
        "\n",
        "    # apply sin to even indices in the array; 2i\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "    # apply cos to odd indices in the array; 2i+1\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_srhAd-pgSAt"
      },
      "source": [
        "Masking:                                                                                                         \n",
        "--> Padding mask for masking \"pad\" sequences                          \n",
        "--> Lookahead mask for masking future words from contributing in prediction of current words in self attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qp-ipoljgUcz"
      },
      "source": [
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20D-EBlfg4L_"
      },
      "source": [
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHsc41suhAvF"
      },
      "source": [
        "## **Building the Model  **                                                                                   \n",
        "Scaled Dot Product"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sufdcNz8hIkH"
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)  \n",
        "\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "\n",
        "    output = tf.matmul(attention_weights, v)\n",
        "    return output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZZL8bowhRmz"
      },
      "source": [
        "Multi-Headed Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIARM3thhUjl"
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "        \n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    \n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)\n",
        "        k = self.wk(k)\n",
        "        v = self.wv(v)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)\n",
        "        k = self.split_heads(k, batch_size)\n",
        "        v = self.split_heads(v, batch_size)\n",
        "\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "            q, k, v, mask)\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
        "        output = self.dense(concat_attention)\n",
        "            \n",
        "        return output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxksrVZzheB8"
      },
      "source": [
        "Feed Forward Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXtCUaX5hq_R"
      },
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(dff, activation='relu'),\n",
        "        tf.keras.layers.Dense(d_model)\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDUU0cbciEp0"
      },
      "source": [
        "Fundamental Unit of Transformer encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-P0b4xyiIWR"
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    def call(self, x, training, mask):\n",
        "        attn_output, _ = self.mha(x, x, x, mask)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "        return out2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BC0cxxwjYgX"
      },
      "source": [
        "Fundamental Unit of Transformer decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IR9aZqfUjZBO"
      },
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    \n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(attn2 + out1)\n",
        "\n",
        "        ffn_output = self.ffn(out2)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.layernorm3(ffn_output + out2)\n",
        "\n",
        "        return out3, attn_weights_block1, attn_weights_block2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1CvwAdBjefM"
      },
      "source": [
        "Encoder consisting of multiple EncoderLayer(s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZPJMliRjiLl"
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
        "\n",
        "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "        \n",
        "    def call(self, x, training, mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "    \n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training, mask)\n",
        "    \n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhMRnhCvjpVN"
      },
      "source": [
        "Decoder consisting of multiple DecoderLayer(s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lz4vlgPCjrkU"
      },
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n",
        "\n",
        "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
        "    \n",
        "        return x, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHkzp4F6jwt9"
      },
      "source": [
        "Finally, the Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qm_-h9hJjz1y"
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n",
        "\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
        "\n",
        "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "    \n",
        "    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
        "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
        "\n",
        "        dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "\n",
        "        final_output = self.final_layer(dec_output)\n",
        "\n",
        "        return final_output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqOLqoXFj5MN"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lAnFMatj7Jr"
      },
      "source": [
        "# hyper-params\n",
        "num_layers = 4\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "EPOCHS = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VRKVsGzkGO8"
      },
      "source": [
        "Adam optimizer with custom learning rate scheduling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yk6T1FvWkI6J"
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "    \n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgHD7cTskTU9"
      },
      "source": [
        "Defining losses and other metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmFGptCHkTtZ"
      },
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R23Fe_tHkXap"
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebDquHo1kZsU"
      },
      "source": [
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOd04P3OkceM"
      },
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvmVr32hkj2F"
      },
      "source": [
        "Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLoUWquZkkWb"
      },
      "source": [
        "transformer = Transformer(\n",
        "    num_layers, \n",
        "    d_model, \n",
        "    num_heads, \n",
        "    dff,\n",
        "    encoder_vocab_size, \n",
        "    decoder_vocab_size, \n",
        "    pe_input=encoder_vocab_size, \n",
        "    pe_target=decoder_vocab_size,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXGPS9swkq9N"
      },
      "source": [
        "Masks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hiH9xnNkrXa"
      },
      "source": [
        "def create_masks(inp, tar):\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "    dec_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "  \n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nD93QKvNkyW1"
      },
      "source": [
        "Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtV20FYDky1X"
      },
      "source": [
        "checkpoint_path = \"\\checkpoints\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print ('Latest checkpoint restored!!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Fa6_TszlFaN"
      },
      "source": [
        "\n",
        "Training steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2leRRjElFwN"
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, tar):\n",
        "    tar_inp = tar[:, :-1]\n",
        "    tar_real = tar[:, 1:]\n",
        "\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, _ = transformer(\n",
        "            inp, tar_inp, \n",
        "            True, \n",
        "            enc_padding_mask, \n",
        "            combined_mask, \n",
        "            dec_padding_mask\n",
        "        )\n",
        "        loss = loss_function(tar_real, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "    train_loss(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcRbBtW5lIAy",
        "outputId": "52b3d945-6665-4ac2-b38f-15a9fefb9a2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    train_loss.reset_states()\n",
        "  \n",
        "    for (batch, (inp, tar)) in enumerate(dataset):\n",
        "        train_step(inp, tar)\n",
        "    \n",
        "        # 55k samples\n",
        "        # we display 3 batch results -- 0th, middle and last one (approx)\n",
        "        # 55k / 64 ~ 858; 858 / 2 = 429\n",
        "        if batch % 50 == 0:\n",
        "            print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, train_loss.result()))\n",
        "      \n",
        "    \n",
        "    \n",
        "    print ('Epoch {} Loss {:.4f}'.format(epoch + 1, train_loss.result()))\n",
        "\n",
        "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 9.3506\n",
            "Epoch 1 Batch 50 Loss 9.2677\n",
            "Epoch 1 Batch 100 Loss 9.1373\n",
            "Epoch 1 Batch 150 Loss 9.0010\n",
            "Epoch 1 Batch 200 Loss 8.8383\n",
            "Epoch 1 Batch 250 Loss 8.6502\n",
            "Epoch 1 Batch 300 Loss 8.4377\n",
            "Epoch 1 Batch 350 Loss 8.2132\n",
            "Epoch 1 Batch 400 Loss 7.9849\n",
            "Epoch 1 Batch 450 Loss 7.7711\n",
            "Epoch 1 Batch 500 Loss 7.5869\n",
            "Epoch 1 Batch 550 Loss 7.4285\n",
            "Epoch 1 Batch 600 Loss 7.2906\n",
            "Epoch 1 Batch 650 Loss 7.1688\n",
            "Epoch 1 Batch 700 Loss 7.0614\n",
            "Epoch 1 Batch 750 Loss 6.9645\n",
            "Epoch 1 Batch 800 Loss 6.8777\n",
            "Epoch 1 Batch 850 Loss 6.7982\n",
            "Epoch 1 Loss 6.7850\n",
            "Time taken for 1 epoch: 652.8670156002045 secs\n",
            "\n",
            "Epoch 2 Batch 0 Loss 5.4666\n",
            "Epoch 2 Batch 50 Loss 5.4411\n",
            "Epoch 2 Batch 100 Loss 5.4619\n",
            "Epoch 2 Batch 150 Loss 5.4479\n",
            "Epoch 2 Batch 200 Loss 5.4229\n",
            "Epoch 2 Batch 250 Loss 5.3895\n",
            "Epoch 2 Batch 300 Loss 5.3753\n",
            "Epoch 2 Batch 350 Loss 5.3579\n",
            "Epoch 2 Batch 400 Loss 5.3386\n",
            "Epoch 2 Batch 450 Loss 5.3264\n",
            "Epoch 2 Batch 500 Loss 5.3101\n",
            "Epoch 2 Batch 550 Loss 5.2947\n",
            "Epoch 2 Batch 600 Loss 5.2801\n",
            "Epoch 2 Batch 650 Loss 5.2637\n",
            "Epoch 2 Batch 700 Loss 5.2456\n",
            "Epoch 2 Batch 750 Loss 5.2347\n",
            "Epoch 2 Batch 800 Loss 5.2224\n",
            "Epoch 2 Batch 850 Loss 5.2080\n",
            "Epoch 2 Loss 5.2053\n",
            "Time taken for 1 epoch: 640.8857126235962 secs\n",
            "\n",
            "Epoch 3 Batch 0 Loss 4.7817\n",
            "Epoch 3 Batch 50 Loss 4.8504\n",
            "Epoch 3 Batch 100 Loss 4.8330\n",
            "Epoch 3 Batch 150 Loss 4.8311\n",
            "Epoch 3 Batch 200 Loss 4.8190\n",
            "Epoch 3 Batch 250 Loss 4.8055\n",
            "Epoch 3 Batch 300 Loss 4.7933\n",
            "Epoch 3 Batch 350 Loss 4.7858\n",
            "Epoch 3 Batch 400 Loss 4.7812\n",
            "Epoch 3 Batch 450 Loss 4.7773\n",
            "Epoch 3 Batch 500 Loss 4.7698\n",
            "Epoch 3 Batch 550 Loss 4.7692\n",
            "Epoch 3 Batch 600 Loss 4.7628\n",
            "Epoch 3 Batch 650 Loss 4.7621\n",
            "Epoch 3 Batch 700 Loss 4.7531\n",
            "Epoch 3 Batch 750 Loss 4.7502\n",
            "Epoch 3 Batch 800 Loss 4.7480\n",
            "Epoch 3 Batch 850 Loss 4.7417\n",
            "Epoch 3 Loss 4.7389\n",
            "Time taken for 1 epoch: 640.7797222137451 secs\n",
            "\n",
            "Epoch 4 Batch 0 Loss 4.1510\n",
            "Epoch 4 Batch 50 Loss 4.5303\n",
            "Epoch 4 Batch 100 Loss 4.5025\n",
            "Epoch 4 Batch 150 Loss 4.5086\n",
            "Epoch 4 Batch 200 Loss 4.5077\n",
            "Epoch 4 Batch 250 Loss 4.5022\n",
            "Epoch 4 Batch 300 Loss 4.5016\n",
            "Epoch 4 Batch 350 Loss 4.5061\n",
            "Epoch 4 Batch 400 Loss 4.5052\n",
            "Epoch 4 Batch 450 Loss 4.5000\n",
            "Epoch 4 Batch 500 Loss 4.5013\n",
            "Epoch 4 Batch 550 Loss 4.5001\n",
            "Epoch 4 Batch 600 Loss 4.4940\n",
            "Epoch 4 Batch 650 Loss 4.4937\n",
            "Epoch 4 Batch 700 Loss 4.4910\n",
            "Epoch 4 Batch 750 Loss 4.4903\n",
            "Epoch 4 Batch 800 Loss 4.4904\n",
            "Epoch 4 Batch 850 Loss 4.4907\n",
            "Epoch 4 Loss 4.4923\n",
            "Time taken for 1 epoch: 639.7848222255707 secs\n",
            "\n",
            "Epoch 5 Batch 0 Loss 4.3875\n",
            "Epoch 5 Batch 50 Loss 4.3393\n",
            "Epoch 5 Batch 100 Loss 4.3336\n",
            "Epoch 5 Batch 150 Loss 4.3302\n",
            "Epoch 5 Batch 200 Loss 4.3194\n",
            "Epoch 5 Batch 250 Loss 4.3245\n",
            "Epoch 5 Batch 300 Loss 4.3097\n",
            "Epoch 5 Batch 350 Loss 4.3072\n",
            "Epoch 5 Batch 400 Loss 4.3143\n",
            "Epoch 5 Batch 450 Loss 4.3166\n",
            "Epoch 5 Batch 500 Loss 4.3147\n",
            "Epoch 5 Batch 550 Loss 4.3130\n",
            "Epoch 5 Batch 600 Loss 4.3115\n",
            "Epoch 5 Batch 650 Loss 4.3126\n",
            "Epoch 5 Batch 700 Loss 4.3152\n",
            "Epoch 5 Batch 750 Loss 4.3153\n",
            "Epoch 5 Batch 800 Loss 4.3159\n",
            "Epoch 5 Batch 850 Loss 4.3179\n",
            "Epoch 5 Loss 4.3176\n",
            "Time taken for 1 epoch: 639.2522914409637 secs\n",
            "\n",
            "Epoch 6 Batch 0 Loss 4.1270\n",
            "Epoch 6 Batch 50 Loss 4.1506\n",
            "Epoch 6 Batch 100 Loss 4.1474\n",
            "Epoch 6 Batch 150 Loss 4.1484\n",
            "Epoch 6 Batch 200 Loss 4.1356\n",
            "Epoch 6 Batch 250 Loss 4.1266\n",
            "Epoch 6 Batch 300 Loss 4.1255\n",
            "Epoch 6 Batch 350 Loss 4.1175\n",
            "Epoch 6 Batch 400 Loss 4.1192\n",
            "Epoch 6 Batch 450 Loss 4.1182\n",
            "Epoch 6 Batch 500 Loss 4.1117\n",
            "Epoch 6 Batch 550 Loss 4.1114\n",
            "Epoch 6 Batch 600 Loss 4.1110\n",
            "Epoch 6 Batch 650 Loss 4.1109\n",
            "Epoch 6 Batch 700 Loss 4.1093\n",
            "Epoch 6 Batch 750 Loss 4.1079\n",
            "Epoch 6 Batch 800 Loss 4.1084\n",
            "Epoch 6 Batch 850 Loss 4.1073\n",
            "Epoch 6 Loss 4.1079\n",
            "Time taken for 1 epoch: 639.497542142868 secs\n",
            "\n",
            "Epoch 7 Batch 0 Loss 4.0996\n",
            "Epoch 7 Batch 50 Loss 3.9282\n",
            "Epoch 7 Batch 100 Loss 3.9007\n",
            "Epoch 7 Batch 150 Loss 3.9087\n",
            "Epoch 7 Batch 200 Loss 3.8990\n",
            "Epoch 7 Batch 250 Loss 3.8906\n",
            "Epoch 7 Batch 300 Loss 3.8893\n",
            "Epoch 7 Batch 350 Loss 3.8933\n",
            "Epoch 7 Batch 400 Loss 3.8979\n",
            "Epoch 7 Batch 450 Loss 3.9022\n",
            "Epoch 7 Batch 500 Loss 3.9080\n",
            "Epoch 7 Batch 550 Loss 3.9079\n",
            "Epoch 7 Batch 600 Loss 3.9084\n",
            "Epoch 7 Batch 650 Loss 3.9059\n",
            "Epoch 7 Batch 700 Loss 3.9076\n",
            "Epoch 7 Batch 750 Loss 3.9106\n",
            "Epoch 7 Batch 800 Loss 3.9084\n",
            "Epoch 7 Batch 850 Loss 3.9086\n",
            "Epoch 7 Loss 3.9086\n",
            "Time taken for 1 epoch: 638.463282585144 secs\n",
            "\n",
            "Epoch 8 Batch 0 Loss 3.8107\n",
            "Epoch 8 Batch 50 Loss 3.7175\n",
            "Epoch 8 Batch 100 Loss 3.7255\n",
            "Epoch 8 Batch 150 Loss 3.7215\n",
            "Epoch 8 Batch 200 Loss 3.7241\n",
            "Epoch 8 Batch 250 Loss 3.7202\n",
            "Epoch 8 Batch 300 Loss 3.7222\n",
            "Epoch 8 Batch 350 Loss 3.7284\n",
            "Epoch 8 Batch 400 Loss 3.7271\n",
            "Epoch 8 Batch 450 Loss 3.7279\n",
            "Epoch 8 Batch 500 Loss 3.7207\n",
            "Epoch 8 Batch 550 Loss 3.7202\n",
            "Epoch 8 Batch 600 Loss 3.7243\n",
            "Epoch 8 Batch 650 Loss 3.7247\n",
            "Epoch 8 Batch 700 Loss 3.7259\n",
            "Epoch 8 Batch 750 Loss 3.7298\n",
            "Epoch 8 Batch 800 Loss 3.7326\n",
            "Epoch 8 Batch 850 Loss 3.7337\n",
            "Epoch 8 Loss 3.7353\n",
            "Time taken for 1 epoch: 638.6296584606171 secs\n",
            "\n",
            "Epoch 9 Batch 0 Loss 3.7224\n",
            "Epoch 9 Batch 50 Loss 3.5939\n",
            "Epoch 9 Batch 100 Loss 3.5566\n",
            "Epoch 9 Batch 150 Loss 3.5341\n",
            "Epoch 9 Batch 200 Loss 3.5391\n",
            "Epoch 9 Batch 250 Loss 3.5456\n",
            "Epoch 9 Batch 300 Loss 3.5500\n",
            "Epoch 9 Batch 350 Loss 3.5545\n",
            "Epoch 9 Batch 400 Loss 3.5583\n",
            "Epoch 9 Batch 450 Loss 3.5587\n",
            "Epoch 9 Batch 500 Loss 3.5598\n",
            "Epoch 9 Batch 550 Loss 3.5598\n",
            "Epoch 9 Batch 600 Loss 3.5666\n",
            "Epoch 9 Batch 650 Loss 3.5696\n",
            "Epoch 9 Batch 700 Loss 3.5677\n",
            "Epoch 9 Batch 750 Loss 3.5732\n",
            "Epoch 9 Batch 800 Loss 3.5761\n",
            "Epoch 9 Batch 850 Loss 3.5793\n",
            "Epoch 9 Loss 3.5804\n",
            "Time taken for 1 epoch: 638.9598841667175 secs\n",
            "\n",
            "Epoch 10 Batch 0 Loss 3.3811\n",
            "Epoch 10 Batch 50 Loss 3.4458\n",
            "Epoch 10 Batch 100 Loss 3.4349\n",
            "Epoch 10 Batch 150 Loss 3.4280\n",
            "Epoch 10 Batch 200 Loss 3.4302\n",
            "Epoch 10 Batch 250 Loss 3.4279\n",
            "Epoch 10 Batch 300 Loss 3.4279\n",
            "Epoch 10 Batch 350 Loss 3.4259\n",
            "Epoch 10 Batch 400 Loss 3.4256\n",
            "Epoch 10 Batch 450 Loss 3.4237\n",
            "Epoch 10 Batch 500 Loss 3.4232\n",
            "Epoch 10 Batch 550 Loss 3.4259\n",
            "Epoch 10 Batch 600 Loss 3.4320\n",
            "Epoch 10 Batch 650 Loss 3.4348\n",
            "Epoch 10 Batch 700 Loss 3.4383\n",
            "Epoch 10 Batch 750 Loss 3.4410\n",
            "Epoch 10 Batch 800 Loss 3.4462\n",
            "Epoch 10 Batch 850 Loss 3.4495\n",
            "Epoch 10 Loss 3.4500\n",
            "Time taken for 1 epoch: 638.9913787841797 secs\n",
            "\n",
            "Epoch 11 Batch 0 Loss 3.3000\n",
            "Epoch 11 Batch 50 Loss 3.2852\n",
            "Epoch 11 Batch 100 Loss 3.2801\n",
            "Epoch 11 Batch 150 Loss 3.2945\n",
            "Epoch 11 Batch 200 Loss 3.2924\n",
            "Epoch 11 Batch 250 Loss 3.2919\n",
            "Epoch 11 Batch 300 Loss 3.2988\n",
            "Epoch 11 Batch 350 Loss 3.2995\n",
            "Epoch 11 Batch 400 Loss 3.3042\n",
            "Epoch 11 Batch 450 Loss 3.3051\n",
            "Epoch 11 Batch 500 Loss 3.3050\n",
            "Epoch 11 Batch 550 Loss 3.3055\n",
            "Epoch 11 Batch 600 Loss 3.3117\n",
            "Epoch 11 Batch 650 Loss 3.3173\n",
            "Epoch 11 Batch 700 Loss 3.3239\n",
            "Epoch 11 Batch 750 Loss 3.3271\n",
            "Epoch 11 Batch 800 Loss 3.3293\n",
            "Epoch 11 Batch 850 Loss 3.3326\n",
            "Epoch 11 Loss 3.3329\n",
            "Time taken for 1 epoch: 639.2517263889313 secs\n",
            "\n",
            "Epoch 12 Batch 0 Loss 2.9113\n",
            "Epoch 12 Batch 50 Loss 3.2108\n",
            "Epoch 12 Batch 100 Loss 3.2061\n",
            "Epoch 12 Batch 150 Loss 3.2079\n",
            "Epoch 12 Batch 200 Loss 3.1965\n",
            "Epoch 12 Batch 250 Loss 3.1905\n",
            "Epoch 12 Batch 300 Loss 3.1883\n",
            "Epoch 12 Batch 350 Loss 3.1949\n",
            "Epoch 12 Batch 400 Loss 3.2029\n",
            "Epoch 12 Batch 450 Loss 3.2074\n",
            "Epoch 12 Batch 500 Loss 3.2069\n",
            "Epoch 12 Batch 550 Loss 3.2109\n",
            "Epoch 12 Batch 600 Loss 3.2103\n",
            "Epoch 12 Batch 650 Loss 3.2131\n",
            "Epoch 12 Batch 700 Loss 3.2198\n",
            "Epoch 12 Batch 750 Loss 3.2229\n",
            "Epoch 12 Batch 800 Loss 3.2259\n",
            "Epoch 12 Batch 850 Loss 3.2345\n",
            "Epoch 12 Loss 3.2345\n",
            "Time taken for 1 epoch: 639.0267615318298 secs\n",
            "\n",
            "Epoch 13 Batch 0 Loss 3.0153\n",
            "Epoch 13 Batch 50 Loss 3.1109\n",
            "Epoch 13 Batch 100 Loss 3.0915\n",
            "Epoch 13 Batch 150 Loss 3.1016\n",
            "Epoch 13 Batch 200 Loss 3.0961\n",
            "Epoch 13 Batch 250 Loss 3.0977\n",
            "Epoch 13 Batch 300 Loss 3.0999\n",
            "Epoch 13 Batch 350 Loss 3.1061\n",
            "Epoch 13 Batch 400 Loss 3.1154\n",
            "Epoch 13 Batch 450 Loss 3.1130\n",
            "Epoch 13 Batch 500 Loss 3.1163\n",
            "Epoch 13 Batch 550 Loss 3.1225\n",
            "Epoch 13 Batch 600 Loss 3.1282\n",
            "Epoch 13 Batch 650 Loss 3.1318\n",
            "Epoch 13 Batch 700 Loss 3.1366\n",
            "Epoch 13 Batch 750 Loss 3.1446\n",
            "Epoch 13 Batch 800 Loss 3.1475\n",
            "Epoch 13 Batch 850 Loss 3.1520\n",
            "Epoch 13 Loss 3.1523\n",
            "Time taken for 1 epoch: 638.7367131710052 secs\n",
            "\n",
            "Epoch 14 Batch 0 Loss 2.9876\n",
            "Epoch 14 Batch 50 Loss 3.0315\n",
            "Epoch 14 Batch 100 Loss 3.0131\n",
            "Epoch 14 Batch 150 Loss 3.0235\n",
            "Epoch 14 Batch 200 Loss 3.0207\n",
            "Epoch 14 Batch 250 Loss 3.0161\n",
            "Epoch 14 Batch 300 Loss 3.0176\n",
            "Epoch 14 Batch 350 Loss 3.0269\n",
            "Epoch 14 Batch 400 Loss 3.0343\n",
            "Epoch 14 Batch 450 Loss 3.0382\n",
            "Epoch 14 Batch 500 Loss 3.0399\n",
            "Epoch 14 Batch 550 Loss 3.0429\n",
            "Epoch 14 Batch 600 Loss 3.0464\n",
            "Epoch 14 Batch 650 Loss 3.0522\n",
            "Epoch 14 Batch 700 Loss 3.0565\n",
            "Epoch 14 Batch 750 Loss 3.0607\n",
            "Epoch 14 Batch 800 Loss 3.0641\n",
            "Epoch 14 Batch 850 Loss 3.0706\n",
            "Epoch 14 Loss 3.0716\n",
            "Time taken for 1 epoch: 639.1437120437622 secs\n",
            "\n",
            "Epoch 15 Batch 0 Loss 2.8234\n",
            "Epoch 15 Batch 50 Loss 2.9611\n",
            "Epoch 15 Batch 100 Loss 2.9369\n",
            "Epoch 15 Batch 150 Loss 2.9318\n",
            "Epoch 15 Batch 200 Loss 2.9465\n",
            "Epoch 15 Batch 250 Loss 2.9510\n",
            "Epoch 15 Batch 300 Loss 2.9500\n",
            "Epoch 15 Batch 350 Loss 2.9505\n",
            "Epoch 15 Batch 400 Loss 2.9567\n",
            "Epoch 15 Batch 450 Loss 2.9595\n",
            "Epoch 15 Batch 500 Loss 2.9643\n",
            "Epoch 15 Batch 550 Loss 2.9699\n",
            "Epoch 15 Batch 600 Loss 2.9735\n",
            "Epoch 15 Batch 650 Loss 2.9763\n",
            "Epoch 15 Batch 700 Loss 2.9838\n",
            "Epoch 15 Batch 750 Loss 2.9872\n",
            "Epoch 15 Batch 800 Loss 2.9914\n",
            "Epoch 15 Batch 850 Loss 2.9946\n",
            "Epoch 15 Loss 2.9947\n",
            "Time taken for 1 epoch: 639.0516114234924 secs\n",
            "\n",
            "Epoch 16 Batch 0 Loss 2.9206\n",
            "Epoch 16 Batch 50 Loss 2.8705\n",
            "Epoch 16 Batch 100 Loss 2.8605\n",
            "Epoch 16 Batch 150 Loss 2.8621\n",
            "Epoch 16 Batch 200 Loss 2.8608\n",
            "Epoch 16 Batch 250 Loss 2.8583\n",
            "Epoch 16 Batch 300 Loss 2.8716\n",
            "Epoch 16 Batch 350 Loss 2.8751\n",
            "Epoch 16 Batch 400 Loss 2.8841\n",
            "Epoch 16 Batch 450 Loss 2.8957\n",
            "Epoch 16 Batch 500 Loss 2.8944\n",
            "Epoch 16 Batch 550 Loss 2.9007\n",
            "Epoch 16 Batch 600 Loss 2.9017\n",
            "Epoch 16 Batch 650 Loss 2.9054\n",
            "Epoch 16 Batch 700 Loss 2.9119\n",
            "Epoch 16 Batch 750 Loss 2.9140\n",
            "Epoch 16 Batch 800 Loss 2.9185\n",
            "Epoch 16 Batch 850 Loss 2.9212\n",
            "Epoch 16 Loss 2.9229\n",
            "Time taken for 1 epoch: 639.3162512779236 secs\n",
            "\n",
            "Epoch 17 Batch 0 Loss 2.9613\n",
            "Epoch 17 Batch 50 Loss 2.8240\n",
            "Epoch 17 Batch 100 Loss 2.7993\n",
            "Epoch 17 Batch 150 Loss 2.7857\n",
            "Epoch 17 Batch 200 Loss 2.7828\n",
            "Epoch 17 Batch 250 Loss 2.7896\n",
            "Epoch 17 Batch 300 Loss 2.7876\n",
            "Epoch 17 Batch 350 Loss 2.7941\n",
            "Epoch 17 Batch 400 Loss 2.7972\n",
            "Epoch 17 Batch 450 Loss 2.8081\n",
            "Epoch 17 Batch 500 Loss 2.8133\n",
            "Epoch 17 Batch 550 Loss 2.8196\n",
            "Epoch 17 Batch 600 Loss 2.8260\n",
            "Epoch 17 Batch 650 Loss 2.8265\n",
            "Epoch 17 Batch 700 Loss 2.8322\n",
            "Epoch 17 Batch 750 Loss 2.8383\n",
            "Epoch 17 Batch 800 Loss 2.8442\n",
            "Epoch 17 Batch 850 Loss 2.8487\n",
            "Epoch 17 Loss 2.8501\n",
            "Time taken for 1 epoch: 639.5935754776001 secs\n",
            "\n",
            "Epoch 18 Batch 0 Loss 2.6209\n",
            "Epoch 18 Batch 50 Loss 2.7285\n",
            "Epoch 18 Batch 100 Loss 2.7333\n",
            "Epoch 18 Batch 150 Loss 2.7333\n",
            "Epoch 18 Batch 200 Loss 2.7425\n",
            "Epoch 18 Batch 250 Loss 2.7357\n",
            "Epoch 18 Batch 300 Loss 2.7323\n",
            "Epoch 18 Batch 350 Loss 2.7364\n",
            "Epoch 18 Batch 400 Loss 2.7429\n",
            "Epoch 18 Batch 450 Loss 2.7535\n",
            "Epoch 18 Batch 500 Loss 2.7538\n",
            "Epoch 18 Batch 550 Loss 2.7568\n",
            "Epoch 18 Batch 600 Loss 2.7619\n",
            "Epoch 18 Batch 650 Loss 2.7678\n",
            "Epoch 18 Batch 700 Loss 2.7683\n",
            "Epoch 18 Batch 750 Loss 2.7733\n",
            "Epoch 18 Batch 800 Loss 2.7797\n",
            "Epoch 18 Batch 850 Loss 2.7850\n",
            "Epoch 18 Loss 2.7863\n",
            "Time taken for 1 epoch: 639.7416253089905 secs\n",
            "\n",
            "Epoch 19 Batch 0 Loss 2.4703\n",
            "Epoch 19 Batch 50 Loss 2.6933\n",
            "Epoch 19 Batch 100 Loss 2.6622\n",
            "Epoch 19 Batch 150 Loss 2.6588\n",
            "Epoch 19 Batch 200 Loss 2.6635\n",
            "Epoch 19 Batch 250 Loss 2.6686\n",
            "Epoch 19 Batch 300 Loss 2.6690\n",
            "Epoch 19 Batch 350 Loss 2.6762\n",
            "Epoch 19 Batch 400 Loss 2.6803\n",
            "Epoch 19 Batch 450 Loss 2.6833\n",
            "Epoch 19 Batch 500 Loss 2.6864\n",
            "Epoch 19 Batch 550 Loss 2.6913\n",
            "Epoch 19 Batch 600 Loss 2.6936\n",
            "Epoch 19 Batch 650 Loss 2.6981\n",
            "Epoch 19 Batch 700 Loss 2.7051\n",
            "Epoch 19 Batch 750 Loss 2.7098\n",
            "Epoch 19 Batch 800 Loss 2.7159\n",
            "Epoch 19 Batch 850 Loss 2.7225\n",
            "Epoch 19 Loss 2.7243\n",
            "Time taken for 1 epoch: 639.9493262767792 secs\n",
            "\n",
            "Epoch 20 Batch 0 Loss 2.4501\n",
            "Epoch 20 Batch 50 Loss 2.6270\n",
            "Epoch 20 Batch 100 Loss 2.6331\n",
            "Epoch 20 Batch 150 Loss 2.6315\n",
            "Epoch 20 Batch 200 Loss 2.6171\n",
            "Epoch 20 Batch 250 Loss 2.6105\n",
            "Epoch 20 Batch 300 Loss 2.6186\n",
            "Epoch 20 Batch 350 Loss 2.6266\n",
            "Epoch 20 Batch 400 Loss 2.6332\n",
            "Epoch 20 Batch 450 Loss 2.6337\n",
            "Epoch 20 Batch 500 Loss 2.6332\n",
            "Epoch 20 Batch 550 Loss 2.6323\n",
            "Epoch 20 Batch 600 Loss 2.6382\n",
            "Epoch 20 Batch 650 Loss 2.6411\n",
            "Epoch 20 Batch 700 Loss 2.6447\n",
            "Epoch 20 Batch 750 Loss 2.6494\n",
            "Epoch 20 Batch 800 Loss 2.6567\n",
            "Epoch 20 Batch 850 Loss 2.6633\n",
            "Epoch 20 Loss 2.6653\n",
            "Time taken for 1 epoch: 639.6966211795807 secs\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ebyBbreh_Js"
      },
      "source": [
        "### Inference ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-BMaZWLiqv9"
      },
      "source": [
        "**Predicting one word at a time at the decoder and appending it to the output; then taking the complete sequence as an input to the decoder and repeating until maxlen or stop keyword appears**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Qr3GJrCiGp1"
      },
      "source": [
        "def evaluate(input_document):\n",
        "    input_document = document_tokenizer.texts_to_sequences([input_document])\n",
        "    input_document = tf.keras.preprocessing.sequence.pad_sequences(input_document, maxlen=encoder_maxlen, padding='post', truncating='post')\n",
        "\n",
        "    encoder_input = tf.expand_dims(input_document[0], 0)\n",
        "\n",
        "    decoder_input = [summary_tokenizer.word_index[\"<go>\"]]\n",
        "    output = tf.expand_dims(decoder_input, 0)\n",
        "    \n",
        "    for i in range(decoder_maxlen):\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)\n",
        "\n",
        "        predictions, attention_weights = transformer(\n",
        "            encoder_input, \n",
        "            output,\n",
        "            False,\n",
        "            enc_padding_mask,\n",
        "            combined_mask,\n",
        "            dec_padding_mask\n",
        "        )\n",
        "\n",
        "        predictions = predictions[: ,-1:, :]\n",
        "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "        if predicted_id == summary_tokenizer.word_index[\"<stop>\"]:\n",
        "            return tf.squeeze(output, axis=0), attention_weights\n",
        "\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "    return tf.squeeze(output, axis=0), attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnaXTCLuJqsI"
      },
      "source": [
        "def summarize(input_document):\n",
        "    # not considering attention weights for now, can be used to plot attention heatmaps in the future\n",
        "    summarized = evaluate(input_document=input_document)[0].numpy()\n",
        "    summarized = np.expand_dims(summarized[1:], 0)  # not printing <go> token\n",
        "    return summary_tokenizer.sequences_to_texts(summarized)[0]  # since there is just one translated document"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8Yw59HNjBEX",
        "outputId": "ce2794de-e4d3-4c3c-e7ca-bbf314e1e6ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "summarize('As a foodie, I use a lot of Chinese 5 Spice powder in my daily cooking.  It is impossible to get the amount of 5 spice for this price!  All my dishes taste great!')\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'great product'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsNRuUogj_x5",
        "outputId": "3b4255fd-6113-4bfc-da5b-0c4582ad0d52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "summarize('The hotel was quite good.The view from the hotel was breathtaking and the behaviour of the staff was also good.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'good'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Of9P2zKPkvxy",
        "outputId": "d5dec399-7d5c-4e32-9a83-c63892f54aa9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "summarize('The display and the battery of the phone is really bad but the camera just sucks.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'broken cookies'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mZcp2GplGTL",
        "outputId": "9a59137e-70cb-4622-b814-0d0c41113a5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "summarize('The food packaging was very bad but the taste was okayish')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'terrible'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "An7SGlUIlit2",
        "outputId": "0174c03d-de0e-4240-96c6-308d58d4b142",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "summarize('These are the BEST treats for training and rewarding your dog for being good while grooming.  Lower in calories and loved by all the doggies.  Sweet potatoes seem to be their favorite Wet Noses treat!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'my dogs love these'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNe7Z6RgmU0x",
        "outputId": "aedb1d66-ccae-4688-daee-4c6e0af4a3c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "summarize('I am very satisfied ,product is as advertised, I use it on cereal, with raw vinegar, and as a general sweetner.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'totally satisfied'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7NYmmpnnTJ9",
        "outputId": "f7111bd9-8b80-4398-8287-7bcb2b4497d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "summarize(\n",
        "    \"US-based private equity firm General Atlantic is in talks to invest about \\\n",
        "    $850 million to $950 million in Reliance Industries' digital unit Jio \\\n",
        "    Platforms, the Bloomberg reported. Saudi Arabia's $320 billion sovereign \\\n",
        "    wealth fund is reportedly also exploring a potential investment in the \\\n",
        "    Mukesh Ambani-led company. The 'Public Investment Fund' is looking to \\\n",
        "    acquire a minority stake in Jio Platforms.\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'do not buy from this company'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r714XgYn1JTG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}